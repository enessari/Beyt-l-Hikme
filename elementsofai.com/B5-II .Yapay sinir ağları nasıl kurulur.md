

![img](data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8"?>
<svg width="923px" height="230px" viewBox="0 0 923 230" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
    <!-- Generator: Sketch 49 (51002) - http://www.bohemiancoding.com/sketch -->
    <title>Artboard 2 Copy</title>
    <desc>Created with Sketch.</desc>
    <defs></defs>
    <g id="Artboard-2-Copy" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd">
        <g id="Group-22-Copy-3" transform="translate(2.000000, 25.000000)">
            <g id="Group-6">
                <g id="Group-4">
                    <g id="Group-16-Copy-3" transform="translate(41.340224, 26.000000)">
                        <path d="M397.4152,186.805556 L423.262156,129.048428 L433.656216,113.822368 L469.148811,4.40350877 L575.688705,107.700567 L582.719524,20.9829955 L482.356441,4.40350877 L447.36383,8.83059211 L333.901809,56.8792032 L429.786781,120.930556 M508.781976,186.805556 L437.469753,129.048428 L442.804977,120.883955 L569.257244,109.522661 L571.566172,109.522661 L544.105916,182.42818 M632.393583,182.42818 L575.688705,109.522661" id="Path-66" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M316.989334,59.2807018 L201.148512,72.4718567" id="Path-8" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M267.928873,124.963085 L192.403464,73.2671784" id="Path-8" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M100.639471,36.1977339 L192.403464,73.2671784" id="Path-8" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M245.258829,4.00584795 L192.403464,73.2671784" id="Path-8" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M245.258829,4.00584795 L340.659345,63.6549708" id="Path-8" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M245.258829,4.00584795 L104.543066,31.8421053" id="Path-8" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M97.3880276,37.4093567 L166.553402,181.362573" id="Path-8" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M92.6180017,35.0233918 L0.397502153,153.526316" id="Path-8" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M267.928873,124.963085 L332.324222,58.9513889" id="Path-8" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M349.814317,225.968933 L332.324222,58.9513889" id="Path-8" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M267.928873,124.963085 L171.733352,176.658991" id="Path-8" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M271.159776,126.5 L267.928873,215.629751" id="Path-8" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M572.23851,109.13432 L677.188395,90.4256213" id="Path-2" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M581.54565,22.2982456 L677.188395,90.4256213" id="Path-2" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M676.151163,81.9473684 L665.816107,0.824561404" id="Path-2" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M807.326873,91.4912281 L665.816107,0.824561404" id="Path-2" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M808.916882,92.2865497 L838.332041,191.701754" id="Path-2" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M808.916882,92.2865497 L716.696382,171.023392" id="Path-2" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M805.736865,93.0818713 L680.126184,90.6959064" id="Path-2" stroke="#D0E0E5" stroke-width="9"></path>
                        <path d="M707.156331,151.935673 L680.126184,90.6959064" id="Path-2" stroke="#D0E0E5" stroke-width="9"></path>
                        <ellipse id="Oval-7" fill="#1274BF" cx="427.712317" cy="119.725146" rx="37.3652024" ry="37.380117"></ellipse>
                        <ellipse id="Oval-7-Copy" fill="#1274BF" cx="715.489262" cy="171.435673" rx="25.4255238" ry="25.4356725"></ellipse>
                        <ellipse id="Oval-7-Copy-4" fill="#1274BF" cx="839.127046" cy="200.450292" rx="42.5327304" ry="42.5497076"></ellipse>
                        <ellipse id="Oval-7-Copy-12" fill="#1274BF" cx="807.326873" cy="91.4912281" rx="20.2726098" ry="20.2807018"></ellipse>
                        <ellipse id="Oval-7-Copy-2" fill="#1274BF" cx="333.504307" cy="58.0877193" rx="31.4026701" ry="31.4152047"></ellipse>
                        <ellipse id="Oval-7-Copy-3" fill="#1274BF" cx="160.988372" cy="177.385965" rx="31.4026701" ry="31.4152047"></ellipse>
                        <ellipse id="Oval-7-Copy-6" fill="#1274BF" cx="580.750646" cy="19.9122807" rx="16.2975883" ry="16.3040936"></ellipse>
                        <ellipse id="Oval-7-Copy-5" fill="#1274BF" cx="571.608096" cy="109.385965" rx="26.2351421" ry="26.245614"></ellipse>
                        <ellipse id="Oval-7-Copy-10" fill="#1274BF" cx="678.933678" cy="91.8888889" rx="20.670112" ry="20.6783626"></ellipse>
                    </g>
                    <ellipse id="Oval-7-Copy-7" fill="#1274BF" cx="508.405254" cy="28.4152047" rx="28.2226529" ry="28.2339181"></ellipse>
                    <ellipse id="Oval-7-Copy-9" fill="#1274BF" cx="708.746339" cy="28.4152047" rx="28.2226529" ry="28.2339181"></ellipse>
                    <ellipse id="Oval-7-Copy-8" fill="#1274BF" cx="233.731266" cy="98.8011696" rx="23.8501292" ry="23.8596491"></ellipse>
                    <ellipse id="Oval-7-Copy-20" fill="#1274BF" cx="284.611542" cy="29.6081871" rx="15.1050818" ry="15.1111111"></ellipse>
                    <ellipse id="Oval-7-Copy-18" fill="#1274BF" cx="138.728252" cy="59.4327485" rx="31.4026701" ry="31.4152047"></ellipse>
                    <ellipse id="Oval-7-Copy-19" fill="#1274BF" cx="36.9677003" cy="186.684211" rx="36.9677003" ry="36.9824561"></ellipse>
                    <ellipse id="Oval-7-Copy-11" fill="#1274BF" cx="311.641688" cy="152.087719" rx="22.3217054" ry="22.3306153"></ellipse>
                </g>
            </g>
        </g>
    </g>
</svg>)



## II .Yapay sinir ağları nasıl kurulur?

### Daha önce de söylediğimiz gibi, nöronlar çok basit işlem birimleridir. Bölüm 4'teki doğrusal ve lojistik gerilemeyi tartıştıktan sonra, sinir ağlarının temel teknik detayları aynı fikrin küçük farklılıkları olarak görülebilir.

> Not
>
> ## Ağırlıklar ve girdiler
>
> Temel yapay nöron modeli, doğrusal ve lojistik regresyonda olduğu gibi ağırlıklar olarak adlandırılan bir dizi uyarlamalı parametre içerir. Tıpkı regresyonda olduğu gibi, bu ağırlıklar eklenen nöronun girişlerinde çarpanlar olarak kullanılır. Girişlerin ağırlıklarının toplamı girişlerin doğrusal birleşimi olarak adlandırılır. Muhtemelen alışveriş faturası benzetmesini hatırlayabilirsiniz: her bir ürünün miktarını birim fiyatıyla çarparak toplamı elde etmek için eklersiniz.
>

Altı girişli (altı alışveriş ürününün miktarına benzer: patatesler, havuçlar vb.), Giriş1, giriş2, giriş3, giriş4, giriş5 ve giriş6 gibi bir nöronumuz varsa, altı ağırlığa da ihtiyacımız var. Ağırlıklar, ürünlerin fiyatlarına benzer. Onlara kilo1, ağırlık2, ağırlık3, ağırlık4, ağırlık5 ve ağırlık6 diyeceğiz. Ek olarak, genellikle doğrusal regresyonda yaptığımız gibi bir kesişme terimi de dahil etmek isteriz. Bu, örneğin bir kredi kartı ödemesinin işlenmesi nedeniyle sabit bir ek ücret olarak düşünülebilir.

Daha sonra şu gibi lineer kombinasyonu hesaplayabiliriz: linear kombinasyonu = kesişme + ağırlık1 × giriş1 + ... + ağırlık6 × giriş6 (..., toplamın 1'den 6'ya kadar olan tüm terimleri içerdiği anlamına gelen kısa bir gösterimdir).

Bazı örnek sayılarla daha sonra alabiliriz:

10,0 + 5,4 × 8 + (-10,2) × 5 + (-0,1) × 22 + 101,4 × (-5) + 0,0 × 2 + 12,0 × (-3) = -543,0





------

### Alıştırmalar için ilgili sayfaya gidebilirsiniz.

https://course.elementsofai.com/5/2

------





Ağırlıklar neredeyse her zaman, daha önce tartışıldığı gibi doğrusal veya lojistik regresyondaki fikirleri kullanarak aynı fikirlerden öğrenilir. Fakat bunu daha ayrıntılı olarak tartışmadan önce, bir nöronun bir çıkış sinyali göndermeden önce tamamladığı bir başka önemli aşamayı tanıtırız.

### Aktivasyonlar ve çıkışlar

Doğrusal kombinasyon hesaplandıktan sonra, nöron bir işlem daha yapar. Doğrusal kombinasyonu alır ve bunu aktivasyon fonksiyonu denilen bir hale getirir. Aktivasyon fonksiyonunun tipik örnekleri:

- identity işlevi: hiçbir şey yapmayan ve sadece doğrusal birleşimi çıktılayan
- adım işlevi: Doğrusal kombinasyonun değeri sıfırdan büyükse, bir puls (ON) gönderin, aksi halde hiçbir şey yapmayın (OFF).
- sigmoid işlevi: adım işlevinin “yumuşak” bir sürümü

İlk etkinleştirme işlevi olan kimlik işlevinde nöronun doğrusal regresyonla tamamen aynı olduğunu unutmayın. Kimlik fonksiyonunun sinir ağlarında nadiren kullanılmasının nedeni budur: yeni ve ilginç bir şey olmaz.

> Not
>
> ## Nöronlar nasıl aktive edilir
>
> Gerçek biyolojik nöronlar, “çiviler” adı verilen keskin, elektriksel darbeler göndererek iletişim kurar, böylece herhangi bir zamanda, giden sinyalleri açık veya kapalı olur (1 veya 0). Adım işlevi bu davranışı taklit eder. Bununla birlikte, yapay sinir ağları, ikinci tür aktivasyon fonksiyonlarını kullanma eğilimindedir, böylece her zaman sürekli bir sayısal aktivasyon seviyesi üretirler. Bu nedenle, biraz garip bir konuşma şekli kullanmak için, gerçek nöronlar Mors koduna benzer bir şeyle iletişim kurar, oysa yapay nöronlar seslerinin perdesini ses çıkarıyormuş gibi ayarlayarak iletişim kurar.
>

![carstop](data:image/svg+xml;base64,<svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 691.3 428">
    <style>
        .st0{opacity:.5}.st1{fill:#342d71}.st2{fill:#3b3b75}.st3{fill:#0b0146}.st4{fill:#e9e8f0}.st5{fill:#94a7ca}.st6{fill:#eff0f4}.st7{fill:#ffc07f}.st8{fill:#c0c0d8}.st9{fill:#404060}.st10{fill:#d0d0dd}.st11{fill:#ff3e57}.st12{fill:#fff}.st13{fill:#e9e9ed}.st14{opacity:.1}.st15{fill:#4747a1}
    </style>
    <title>
        Artboard 1
    </title>
    <g class="st0">
        <path class="st1" d="M406.8 339.3c0 .9.7 1.7 1.7 1.7h211.3c.9 0 1.7-.7 1.7-1.7 0-2.9-2.3-5.2-5.2-5.2H412.1c-2.9-.1-5.3 2.3-5.3 5.2z"/>
    </g>
    <g class="st0">
        <path class="st1" d="M393 371.8c-193.9 23.8-223.5 26.5-362.4 16.5-12.1-.9-25.4-5.3-28.1-11.7-1.9-4.3 1.9-11.1 9-13.4s15.5-2.9 23.7-3.5c104.2-7.3 209.7-9.7 313.9-2.5 11.3.8 23 1.8 32.7 5s15.7-.2 14 5.8l-2.8 3.8"/>
    </g>
    <path class="st2" d="M312.1 279.4l-141.7 6.1 30.8 79.1 49.7.1 17-2.3z"/>
    <path class="st3" d="M232.3 303.1c-.8 0-1.6.1-2.3.2-4.3.1-11.2.2-13.9.2H215c-15.8.3-29.2 18.9-30 43.2-.8 24.4 10.8 43.5 26.6 44 0 0 16.9.5 20.9.6h1.6c15.8-.3 28.2-20.3 27.7-44.7-.5-24.3-13.7-43.8-29.5-43.5zm18.2 43.8c.3 14.8-7.2 26.9-16.8 27.1h-1c-9.2-.6-16.6-12.1-16.9-26.4-.2-11.8 4.5-21.9 11.2-25.6 1.3-.7 2.7-1.2 4.1-1.4.5-.1.9-.1 1.4-.1 9.7-.2 17.7 11.6 18 26.4z"/>
    <path class="st4" d="M232.6 320.5c-.5 0-1 .1-1.4.1-1.4.2-2.8.7-4.1 1.4-6.8 3.7-11.5 13.8-11.2 25.6.3 14.3 7.8 25.8 16.9 26.4h1c9.6-.2 17.1-12.3 16.8-27.1-.4-14.8-8.4-26.6-18-26.4z"/>
    <path class="st5" d="M46.1 296.2c.8 0 21.1-.9 21.1-.9l7.5 11.9-12.4 15.6-17.5-5.8 1.3-20.8z"/>
    <path class="st3" d="M39.9 334.1c2.2-24.3 16.7-42.8 32.4-41.4 2.9.3 5.6 1.2 8.1 2.6-11.5 5.5-20.2 21.7-20.9 41-.9 23.5 10.4 43.2 25.4 45-4-.3-20.9-1.8-20.9-1.8-15.8-1.3-26.3-21.1-24.1-45.4z"/>
    <path class="st3" d="M57.8 336.2c.7-19.4 9.4-35.5 20.9-41 2.9-1.4 6.1-2.1 9.3-2 15.8.6 27.8 20.8 26.9 45.2-.9 24.4-14.4 43.6-30.2 43.1-.5 0-1.1-.1-1.6-.1-14.9-2-26.1-21.7-25.3-45.2z"/>
    <path class="st2" d="M420.8 317.1l-9.9-45.2-95.1 3.1 3.7 84.7 44.3-3.2 53.5-15.7z"/>
    <path class="st3" d="M389.3 283.8c-3 0-5.8.7-8.5 2-2.7-1.6-5.7-2.5-8.8-2.7-15.8-.9-29.6 18.2-31 42.5-1.3 24.3 9.8 45.7 25.6 46.5.1 0-.1 0 0 0 3.2.3 12.8-.1 17.9.1.7.1 3.8-.1 4.6-.1 15.8 0 28.7-19.7 28.7-44.1.1-24.3-12.7-44.1-28.5-44.2z"/>
    <path class="st4" d="M375.4 327.9c0-10.2 4.3-18.9 10.3-22 1.2-.6 2.4-1 3.6-1.2.4-.1.8-.1 1.2-.1 8.3 0 15.1 10.5 15 23.3 0 12.9-6.8 23.3-15.2 23.3h-.8c-7.9-.7-14.2-10.9-14.1-23.3z"/>
    <path class="st6" d="M47.4 222.5c11.2-27.3 22-97.6 226.8-81.6 0 0 74.3 5.4 118 87s26.5 111.3 26.5 111.3-.3.6-1.3 1.5c1.2-16.5 2.5-62.5-18.9-64.1-21.6-1.6-31.2 54.9-33.8 74.2-.5 3.4-3.1 6-6.5 6.4-15.4 2-34.8 3.6-59 4.7-13.8.6-26.8 1.3-39.1 2 3.1-6.6 10.7-27.4.5-52.2-11.9-28.7-33.5-22.8-33.5-22.8s-20.1 3.4-33.2 57.4c-1.1 4.5-2.5 8.5-4.1 12-3.2 7.1-10.4 11.6-18.2 11.2-46.6-2.1-83.9-1.7-124.4-4.8-56.5-4.2-11.3-114.1.2-142.2z"/>
    <path class="st7" d="M183.2 206.9c-8.2 0-14.9 6.7-14.9 14.9v18h14.3c7.1 0 13.2-5 14.6-12l.6-3.1c1.8-9.3-5.3-17.8-14.6-17.8z"/>
    <path class="st7" d="M182.6 240.1h-14.7v-18.4c0-8.4 6.9-15.3 15.3-15.3 4.6 0 8.9 2 11.8 5.6 2.9 3.5 4.1 8.2 3.2 12.7l-.6 3.1c-1.4 7.1-7.7 12.3-15 12.3zm-13.9-.8h13.8c6.9 0 12.9-4.9 14.2-11.7l.6-3.1c.8-4.2-.3-8.6-3-11.9-2.8-3.3-6.8-5.3-11.1-5.3-8 0-14.4 6.5-14.4 14.4v17.6h-.1z"/>
    <path class="st8" d="M244.6 233.7c-1.7-6.1-3.1-11.5-3.9-15.5-3.9-21.3-8.9-48.6 12.4-53.7s56.6-9.6 73.1 17.5c16.2 26.6 20.5 45.9 20.5 45.9s.3.9.7 2.5M168.3 239.7c3.9 0 7.3-2.8 8.1-6.6 13.9-69.7 10.6-65.9-58.5-65.3-41.3.4-54.5 35.5-59.1 46.5-1.7 4.1-4.5 10.9-7.3 19-1 2.7 1.1 5.6 4 5.6l112.8.8z"/>
    <path class="st10" d="M619.4 126.2h-3.3c-2 0-3.6 1.6-3.6 3.6v207.4c0 2 1.6 3.6 3.6 3.6h3.3c2 0 3.6-1.6 3.6-3.6V129.8c0-2-1.6-3.6-3.6-3.6z"/>
    <path class="st11" d="M636.6 3.5H599c-11.4 0-21.9 6.1-27.6 15.9l-18.8 32.5c-5.7 9.9-5.7 22 0 31.8l18.8 32.5c5.7 9.9 16.2 15.9 27.6 15.9h37.6c11.4 0 21.9-6.1 27.6-15.9L683 83.7c5.7-9.9 5.7-22 0-31.8l-18.8-32.5C658.5 9.6 648 3.5 636.6 3.5z"/>
    <path class="st6" d="M639 135.7h-42.3c-11.1 0-21.5-6-27-15.6l-21.2-36.7c-5.6-9.6-5.6-21.6 0-31.2l21.2-36.7C575.2 6 585.5 0 596.6 0H639c11.1 0 21.5 6 27 15.6l21.2 36.7c5.6 9.6 5.6 21.6 0 31.2L666 120.1c-5.6 9.7-15.9 15.6-27 15.6zM596.6 7c-8.6 0-16.7 4.6-21 12.1l-21.2 36.7c-4.3 7.5-4.3 16.7 0 24.2l21.2 36.7c4.3 7.5 12.3 12.1 21 12.1H639c8.6 0 16.7-4.6 21-12.1L681.1 80c4.3-7.5 4.3-16.7 0-24.2l-21.2-36.7C655.6 11.6 647.6 7 639 7h-42.4z"/>
    <g>
        <path class="st12" d="M578.8 72.2c0 3.6 1.5 5.7 3.6 5.7 1.7 0 3-1.6 3-3.9 0-2.7-1.6-3.6-3.5-4.6-4.2-2.2-7.7-4.2-7.7-10.9 0-6.1 3.8-9.3 8-9.3 5.4 0 8.1 4.9 8.1 10.6H585c0-2.9-1-4.8-2.8-4.8-1.5 0-2.8 1.1-2.8 3.4 0 2.2 1.3 3.4 3.3 4.4 3.7 1.9 8 4.1 8 10.7 0 6.8-3.5 10.6-8.6 10.6-4.9 0-8.6-4.3-8.6-11.8l5.3-.1zM599.4 56.7h-5.1V50h15.6v6.6h-5.2v26.5h-5.3V56.7zM612.8 66.6c0-9.1 4.5-17.4 12.1-17.4s12 8.3 12 17.4-4.5 17.4-12 17.4c-7.6.1-12.1-8.3-12.1-17.4zm18.7 0c0-5.4-2.5-10.5-6.7-10.5s-6.7 5.1-6.7 10.5 2.5 10.5 6.7 10.5 6.7-5 6.7-10.5zM643.1 50h9.5c5 0 8.3 5.5 8.3 12 0 6.4-3.3 12-8.3 12h-4.3v9.2H643l.1-33.2zm12.5 12c0-3.1-1.4-5.7-3.6-5.7h-3.6v11.5h3.6c2.2 0 3.6-2.7 3.6-5.8z"/>
    </g>
    <g>
        <path class="st2" d="M244.6 233.7l102.9-3.3c-.5-1.6-.7-2.5-.7-2.5s-.1-.3-.2-.9c-8 .1-72.1-2-104.5-3.1.6 3 1.5 6.3 2.5 9.8z"/>
    </g>
    <g>
        <path class="st3" d="M270.8 259.2c-10.4 0-18.8-8.4-18.8-18.8v-7.5c0-10.4 8.4-18.8 18.8-18.8s18.8 8.4 18.8 18.8v7.5c0 10.3-8.4 18.8-18.8 18.8zm0-41.7c-8.5 0-15.3 6.9-15.3 15.3v7.5c0 8.5 6.9 15.3 15.3 15.3 8.5 0 15.3-6.9 15.3-15.3v-7.5c0-8.4-6.8-15.3-15.3-15.3z"/>
    </g>
    <g>
        <path class="st13" d="M347.3 229.9l-102.7 3.3c7.3 25.6 21.8 65.2 28.6 81.4 7 16.7 15.5 23.2 47.2 20.5 30.5-2.6 30.6-13.5 32.9-29.6 4.3-30-2.9-65.1-6-75.6z"/>
    </g>
    <g class="st14">
        <path class="st15" d="M308.1 336.5c-22.4 0-29.6-7.2-35.7-21.6-6.8-16.1-21.3-55.9-28.7-81.5-.1-.5.1-.9.6-1.1.5-.1.9.1 1.1.6 7.3 25.5 21.8 65.2 28.6 81.3 6.8 16.3 14.8 22.6 46.3 19.9 28.4-2.4 29.8-12 31.8-26.6.1-.7.2-1.5.3-2.2 4.3-29.8-2.8-64.5-6-75.3-.1-.5.1-.9.6-1.1.5-.1.9.1 1.1.6 3.1 10.9 10.3 45.9 6 76-.1.8-.2 1.5-.3 2.2-2.1 14.8-3.6 25.6-33.4 28.1-4.5.5-8.6.7-12.3.7z"/>
    </g>
</svg>
)



Lineer kombinasyon ve aktivasyon fonksiyonu tarafından belirlenen nöronun çıkışı, bir tahmin veya karar almak için kullanılabilir. Örneğin, ağ, kendi kendini süren bir arabanın önündeki bir durma işaretini tanımlamak için tasarlandıysa, giriş, arabanın önüne tutturulmuş bir kamera tarafından yakalanan bir görüntünün pikselleri olabilir ve çıkış, işaretten önce arabayı durduran bir durdurma prosedürü etkinleştirmek için kullanılabilir.

Ağda öğrenme veya uyarlama, ağların doğru çıktıları üretmesini sağlayacak şekilde, aynı doğrusal veya lojistik regresyonda olduğu gibi ağırlıklar ayarlandığında gerçekleşir. Birçok sinir ağı çok büyüktür ve en büyüğü yüz milyarlarca ağırlık içerir. Hepsini optimize etmek, büyük miktarda hesaplama gücü gerektiren göz korkutucu bir görev olabilir.



------

### Alıştırmalar için ilgili sayfaya gidebilirsiniz.

https://course.elementsofai.com/5/2

------





### Perceptron: Bütün YSA'ların annesi (YSA:Yapay Sinir Ağları)

Perceptron, yukarıda bahsettiğimiz adım etkinleştirme işlevine sahip basit nöron modeli için süslü bir isimdir. Sinirsel hesaplamanın ilk resmi modellerinden biriydi ve sinir ağları tarihindeki temel rolü nedeniyle onu “yapay sinir ağlarının anası” olarak adlandırmak haksızlık olmazdı.

İkili sınıflandırma görevlerinde basit bir sınıflandırıcı olarak kullanılabilir. Perceptron algoritması olarak adlandırılan veriden Perceptron algoritması denilen ağırlıkları öğrenmeye yönelik bir yöntem 1957 yılında psikolog Frank Rosenblatt tarafından tanıtıldı. Perceptron algoritmasını ayrıntılı olarak incelemeyeceğiz. En yakın komşu sınıflandırıcı kadar basit olduğunu söylemek yeterli. Temel ilke, ağ eğitim verilerini her seferinde bir örnek olarak beslemektir. Her bir yanlış sınıflandırma, ağırlıkta bir güncellemeye yol açar.

Not

> ## AI abartısı
>
> Keşfedilmesinden sonra, Perceptron algoritması, mucit Frank Rosenblatt tarafından yapılan iyimser ifadelerden ötürü çok dikkat çekti. AI hyperbole klasik bir örneği, 8 Temmuz 1958'de yayınlanan New York Times makalesidir: 
> “Donanma, bugün yürüyebileceği, konuşabileceği, görebileceği, tekrar üretebileceği ve varoluşunun farkında olabileceğini umduğu bir elektronik bilgisayarın embriyosunu ortaya çıkardı. ” 
>
> Lütfen dikkat edin sinir ağları meraklıları, iyimserliğe yatkın olanların hepsi değil. AI'ya yönelik mantık temelli uzman sistemler yaklaşımının yükselişi ve düşüşü AI-hype'ın aynı özelliklerine sahipti ve insanlar nihai atılımın sadece kısa bir süre sonra olduğunu iddia ettiler. Hem 1960'ların başlarında hem de 1980'lerin sonunda ortaya çıkan sonuç AI Winter denilen araştırma fonunda bir çöküş oldu.
>

Sonunda 1960'larda sinir ağı yaklaşımının yirmi yıldan fazla bir süredir vazgeçilmesinin neredeyse tamamen bitmesine yol açan tartışmaların geçmişi son derece etkileyicidir. Makale *A Sociological Study of the Official History of the Perceptrons Controversy*  yayınlanan Mikel Olazaran tarafından ( *Science Sosyal Bilgiler*, 1996) olayları bilim sosyolojisi açısından inceler. Bugün okumak oldukça düşündürücüdür. Yakında insan zekası seviyesine ulaşacak ve kendini bilinçlendirecek sinir ağları algoritmaları geliştirmiş olan ünlü AI kahramanlarıyla ilgili hikayeler okumak, şu anki yutturma sırasında yapılan bazı ifadelerle karşılaştırılabilir. Yukarıdaki makaleye bakarsanız, hepsini okumamış olsanız bile, bugünün haberleri için ilginç bir arka plan sağlayacaktır. Örneğin [MIT Teknoloji İncelemesinde](https://www.technologyreview.com/s/608911/is-ai-riding-a-one-trick-pony/) bir [makale](https://www.technologyreview.com/s/608911/is-ai-riding-a-one-trick-pony/) düşününEylül 2017'de yayınlanan ve AI için milyonlarca dolarlık bir Vector Vector enstitüsünün kurucu ortağı olan Jordan Jacobs, 1980'lerde sinir ağı algoritmalarının geliştirilmesine katkılarından dolayı Geoffrey Hinton'u (şu anki derin öğrenme patlamasının figür başkanı) Einstein'la karşılaştırdı. ve sonra. Ayrıca önceki bölümde belirtilen İnsan Beyni projesini de hatırlayın.

Hinton'a göre, “çalışmadığı gerçeği sadece geçici bir sıkıntıdır” (makaleye göre, Hinton yukarıdaki ifade hakkında gülüyor olsa da, onunla ilgili ne kadar ciddi olduğunu söylemek zordur). İnsan beyni projesi [“bilinç anlayışımızda derin bir sıçramaya yakın” olduğunu](https://www.humanbrainproject.eu/en/follow-hbp/news/the-quest-for-consciousness/) iddia ediyor . Bu tanıdık gelmiyor mu?

Hiç kimse geleceği kesin olarak bilemez, ancak yakınlardaki gelişmelere ilişkin daha önceki duyuruların kayıtlarını bilerek, bazı eleştirel düşünceler önerilir. Son bölümde AI'nın geleceğine döneceğiz, ancak şimdilik yapay sinir ağlarının nasıl kurulduğunu görelim.

### Nöronları bir araya getirmek: Ağlar

Tek bir nöron, gerçek yaşam uygulamalarının çoğunda güvenilir kararlar ve tahminler almak için çok basit olacaktır. Sinir ağlarının tüm potansiyelini açığa çıkarmak için, bir nöronun çıktısını, başka nöronların çıktısı gibi diğer nöronların girdisi olarak kullanabiliriz. Tüm ağın çıktısı, çıkış katmanı olarak adlandırılan belirli bir nöron alt kümesinin çıktısı olarak elde edilir. Buna, sinir ağlarının parametrelerini verilerden öğrenerek farklı davranışlar üretmeye adapte olma şeklini tartıştıktan sonra, bir miktar geri döneceğiz.

> Anahtar terminoloji
>
> ## Katmanlar
>
> Genellikle ağ mimarisi katmanlardan oluşur. Giriş katmanı, girişlerini doğrudan veriden alan nöronlardan oluşur. Örneğin, bir görüntü tanıma görevinde, giriş katmanı, giriş görüntüsünün piksel değerlerini giriş katmanının girişleri olarak kullanır. Ağ tipik olarak diğer nöronların çıktılarını girdi olarak kullanan ve çıktılarını diğer nöron tabakalarına girdi olarak kullanılan gizli katmanlara da sahiptir. Son olarak, çıktı katmanı tüm ağın çıktısını üretir. Belirli bir katmandaki tüm nöronlar önceki katmandaki nöronlardan girdi alır ve çıktılarını bir sonrakine besler.
>

Çok katmanlı bir ağın klasik bir örneği, çok katmanlı algılayıcı olarak adlandırılır. Yukarıda tartıştığımız gibi, Rosenblatt'ın Perceptron algoritması, bir Perceptronın ağırlıklarını öğrenmek için kullanılabilir. Çok katmanlı algılayıcı için, ilgili öğrenme problemi çok daha zor ve çalışan bir çözüm keşfedilmesi uzun zaman aldı. Fakat nihayetinde bir kişi tarafından icat edildi: geri yayılım algoritması 1980'lerin sonunda sinir ağlarının canlanmasına yol açtı. Hala en gelişmiş derin öğrenme çözümlerinin kalbinde yer almaktadır.

> Not
>
> ## Bu arada Helsinki'de ...
>
> Geri yayılım algoritmasına giden yollar oldukça uzun ve dolambaçlıdır. Tarihin ilginç bir kısmı, Helsinki Üniversitesi bilgisayar bilimleri bölümü ile ilgilidir. Bölümün 1967'de kurulmasından yaklaşık üç yıl sonra, Seppo Linnainmaa adlı bir öğrenci tarafından bir [yüksek lisans tezi](http://people.idsia.ch/~juergen/linnainmaa1970thesis.pdf) hazırlandı. Tezin konusu, “Bireysel yuvarlama hatalarına Taylor yaklaşımı ile algoritmaların kümülatif yuvarlama hatası” (“Cumulative rounding error of algorithms as a Taylor approximation of individual rounding errors” ) idi (tez, Fince olarak yazılmıştır, bu yüzden asıl başlık olan “Algoritmin kumulatiivinen pyöristysvirhe yksittäisten pyöristysvirheiden Taylor-kehitelmänä”).
>
> Tezde geliştirilen otomatik farklılaşma yöntemi daha sonra diğer araştırmacılar tarafından, çok tabakalı bir sinir ağının çıktısının, geri yayılmadaki ana fikir olan bireysel ağırlıklara göre hassasiyetini ölçmek için uygulanmıştır.
>

### Basit bir sinir ağı sınıflandırıcısı

Bir sinir ağı sınıflandırıcı kullanmanın nispeten basit bir örneğini vermek için, MNIST basamak tanıma görevine çok benzeyen, yani görüntüleri iki sınıfta sınıflandıran bir görev düşüneceğiz. Öncelikle, bir görüntünün bir çarpı işareti (x) veya daire (o) gösterip göstermediğini sınıflandırmak için bir sınıflandırıcı oluşturacağız. Resimlerimiz burada renkli veya beyaz pikseller olarak temsil edilir ve pikseller 5x5 ızgarada düzenlenir. Bu formatta haç ve çember (daha çok elmas gibi, dürüst olmak üzere) resimlerimiz şuna benzer:



![1564503799480](assets/1564503799480.png)

Bir sinir ağı sınıflandırıcısı oluşturmak için, problemi öğrendiğimiz yöntemleri kullanarak çözebileceğimiz bir şekilde resmileştirmeliyiz. İlk adımımız, piksellerdeki bilgiyi bir sınıflandırıcıya girdi olarak kullanılabilecek sayısal değerlerle temsil etmektir. Kare renkliyse 1, beyaz ise 0 kullanın. Yukarıdaki grafikteki sembollerin farklı renkte olmasına rağmen (yeşil ve mavi), sınıflandırıcımızın renk bilgisini görmezden geleceğini ve sadece renkli / beyaz bilgileri kullanacağını unutmayın. Görüntüdeki 25 piksel, sınıflandırıcı girdilerimizi oluşturur.

Sayısal gösterimde hangi pikselin hangisi olduğunu bildiğimizden emin olmak için, pikselleri metin okuduğunuzla aynı sırada listelemeye karar verebiliriz, bu nedenle en baştan satır satır satır sıraya ve her satır soldan sağa okunur. Örneğin haçtaki ilk satır 1,0,0,0,1; ikinci sıra 0,1,0,1,0 ve benzeri olarak. Çapraz giriş için tam giriş: 1.0,0,0,1,0,1,0,1,0,0,0,1,0,0,0,1,0,1,0, 1,0,0,0,1.

İlk adımın girdilerin doğrusal bir kombinasyonunu hesaplamak olduğu temel nöron modelini kullanacağız. Bu nedenle, giriş piksellerinin her biri için bir ağırlık gerekir; bu, toplamda 25 ağırlık anlamına gelir.

Son olarak, adım aktivasyon fonksiyonunu kullanıyoruz. Doğrusal kombinasyon negatifse, nöron aktivasyonu sıfırdır, ki bir çarpı işaret etmek için kullanmaya karar veririz. Doğrusal kombinasyon pozitifse, nöron aktivasyonu bir çemberi belirlemeye karar verdiğimiz birdir.

Tüm ağırlıklar aynı sayısal değeri aldığında ne olacağını deneyelim, 1. Bu kurulumda, çapraz görüntü için doğrusal kombinasyonumuz 9 olacaktır (9 renkli piksel, yani 9 × 1 ve 16 beyaz piksel, 16 × 0), ve daire görüntüsü için 8 olacaktır (8 renkli piksel, 8 × 1 ve 17 beyaz piksel, 17 × 0). Başka bir deyişle, doğrusal kombinasyon her iki görüntü için de pozitiftir ve bu nedenle daireler olarak sınıflandırılır. Sınıflandırılacak sadece iki resim olduğu göz önüne alındığında çok iyi bir sonuç değil.

Sonucu geliştirmek için ağırlıkları, lineer kombinasyonun bir çarpı için negatif, bir çember için pozitif olacak şekilde ayarlamamız gerekir. Çaprazların ve dairelerin görüntülerini neyin farklılaştığını düşünürsek, dairelerin görüntünün merkezinde renkli pikselleri olmadığını görebiliriz; Aynı şekilde, görüntünün köşelerindeki pikseller çarpı renkli, daire içinde beyazdır.

Şimdi ağırlıkları ayarlayabiliriz. İşi yapan sonsuz sayıda ağırlık var. Örneğin, ortadaki piksele (13. piksele) ağırlık -1 ve görüntünün dört tarafının her birinin ortasındaki piksellere ağırlık 1 atayın, böylece diğer tüm ağırlıklar 0 olur. Şimdi, çapraz giriş için Merkez piksel –1 değerini verirken, diğer tüm pikseller için ya ağırlığın piksel değeri 0 olur, böylece –1 de toplam değerdir. Bu, aktivasyon 0'a yol açar ve çarpı doğru bir şekilde sınıflandırılır.

Peki o zaman daire? Kenarların ortasındaki piksellerin her biri toplamda 4 × 1 = 4 yapan 1 değerini üretir. Diğer tüm pikseller için ya piksel değeri ya da ağırlık sıfırdır, yani 4 toplamdır. 4 pozitif bir değer olduğundan, aktivasyon 1 olur ve daire de doğru şekilde tanınır.

### Mutlu ya da değil

Şimdi, gülen yüzler için bir sınıflandırıcı oluşturmak için benzer nedenleri takip edeceğiz. Resimdeki giriş piksellerine üzerlerine tıklayarak ağırlık atayabilirsiniz. Bir kez tıklamak, ağırlığı 1 olarak ve tekrar tıklamak ise -1 olarak ayarlar. Etkinleştirme 1, görüntünün mutlu bir yüz olarak sınıflandırıldığını gösterir; bu, doğru olabilir veya olmayabilir;

Tüm gülen yüzleri doğru şekilde sınıflandıramayacağınız için cesaretlendirmeyin: basit sınıflandırıcımız ile bu mümkün değil! Bu önemli bir öğrenme hedefidir: bazen mükemmel sınıflandırma mümkün değildir çünkü sınıflayıcı çok basittir. Bu durumda, girdilerin doğrusal bir kombinasyonunu kullanan basit nöron görev için çok basittir. Farklı durumlarda iyi sonuç veren sınıflandırıcıları nasıl oluşturabileceğinizi gözlemleyin: Bazıları, mutlu yüzlerin çoğunu, üzgün yüzler için daha kötü olurken veya diğer yollarla daha iyi sınıflandırır.

![1564503833811](assets/1564503833811.png)

![1564503841274](assets/1564503841274.png)

------

### Alıştırmalar için ilgili sayfaya gidebilirsiniz.

https://course.elementsofai.com/5/2

------






